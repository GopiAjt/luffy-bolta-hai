import re
import json
import requests
from typing import List, Dict
import os
from dotenv import load_dotenv
import google.generativeai as genai
import logging
import time
import shutil

# Set up logging
logger = logging.getLogger(__name__)
if not logger.hasHandlers():
    handler = logging.StreamHandler()
    formatter = logging.Formatter('%(levelname)s:%(name)s:%(message)s')
    handler.setFormatter(formatter)
    logger.addHandler(handler)
logger.setLevel(logging.INFO)

# Load environment variables from .env
load_dotenv()
api_key = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
if not api_key:
    raise ValueError("Set GOOGLE_API_KEY or GEMINI_API_KEY in your .env file")

genai.configure(api_key=api_key)
model = genai.GenerativeModel("gemini-2.0-flash")


def parse_ass_dialogues(ass_path: str) -> List[Dict]:
    dialogues = []
    dialogue_re = re.compile(
        r"Dialogue: [^,]*,([^,]*),([^,]*),[^,]*,[^,]*,[^,]*,[^,]*,[^,]*,(.*)")
    with open(ass_path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.startswith('Dialogue:'):
                match = dialogue_re.match(line)
                if match:
                    start, end, text = match.groups()
                    text = re.sub(
                        r'{.*?}', '', text).replace('\n', ' ').strip()
                    dialogues.append(
                        {'start': start.strip(), 'end': end.strip(), 'text': text})
    return dialogues


def group_dialogues(dialogues: List[Dict]) -> List[Dict]:
    groups = []
    current = None
    for dlg in dialogues:
        if not current:
            current = {'start': dlg['start'],
                       'end': dlg['end'], 'text': dlg['text']}
        else:
            if re.search(r'[.!?]$', current['text']) or (dlg['text'].lower().startswith('but') or dlg['text'].lower().startswith('and')):
                groups.append(current)
                current = {'start': dlg['start'],
                           'end': dlg['end'], 'text': dlg['text']}
            else:
                current['end'] = dlg['end']
                current['text'] += ' ' + dlg['text']
    if current:
        groups.append(current)
    return groups


def get_gemini_image_description(start, end, summary):
    prompt = (
        "You are an expert video content designer. Your task is to map subtitle segments to suggested background image descriptions, structured in JSON.\n"
        f"Subtitle segment:\n\"{summary}\"\n\n"
        f"Generate a JSON object with:\n- start_time: \"{start}\"\n- end_time: \"{end}\"\n- summary: short subtitle text\n- image_description: concise prompt for a slide background image\n\n"
        "Schema:\n{\"start_time\": \"string\", \"end_time\": \"string\", \"summary\": \"string\", \"image_description\": \"string\"}"
    )
    try:
        response = model.generate_content(prompt)
        if response.text:
            logger.info(f"Gemini raw response: {response.text}")
            import json as _json
            text = response.text.strip()
            if text.startswith("```"):
                # Remove first line (```json or ```)
                text = text.split('\n', 1)[-1]
                if text.endswith("```"):
                    text = text.rsplit('```', 1)[0]
                text = text.strip()
            try:
                return _json.loads(text)
            except Exception as parse_err:
                logger.error(
                    f"Failed to parse Gemini response as JSON: {response.text}")
                raise
        else:
            logger.error("No text generated by Gemini API")
            raise ValueError("No text generated by Gemini API")
    except Exception as e:
        logger.error(f"Error from Gemini API: {str(e)}")
        raise


def filter_irrelevant_summaries(slides):
    # List of phrases that indicate a slide should be merged with the previous
    skip_phrases = [
        "get this.", "it's both.", "trick question.", "but here's the question.",
        "but that's not all.", "trick question", "get this", "it's both", "but here's the question", "but that's not all"
    ]
    filtered = []
    for slide in slides:
        summary = slide.get("summary", "").strip().lower()
        # If summary is in skip_phrases, merge interval with previous
        if any(phrase in summary for phrase in skip_phrases):
            if filtered:
                # Extend the end_time of the previous slide
                filtered[-1]["end_time"] = slide["end_time"]
        else:
            filtered.append(slide)
    return filtered


def generate_gemini_image_slides(ass_path: str, out_path: str, image_dir: str = None) -> str:
    # Read and group all dialogues as before
    dialogues = parse_ass_dialogues(ass_path)
    groups = group_dialogues(dialogues)
    # Prepare a single prompt with all grouped segments
    prompt_segments = []
    for group in groups:
        prompt_segments.append(
            f"[{group['start']} - {group['end']}] {group['text']}")
    all_segments = "\n".join(prompt_segments)
    prompt = (
        "You are an expert video content designer. Your task is to map subtitle segments to suggested Google image search queries, structured in JSON.\n"
        f"Here are the grouped subtitle segments for a short video:\n{all_segments}\n\n"
        "Generate a JSON array where each object has:\n- start_time (string, e.g. '0:00:00')\n- end_time\n- summary (short subtitle text)\n- image_search_query (a concise, highly relevant Google image search string for a One Piece slide, including character names, powers, and the term 'One Piece')\n\n"
        "IMPORTANT TIMING RULES:\n"
        "- PRESERVE the exact start_time and end_time from the original subtitle segments\n"
        "- Do NOT extend or modify timing to cover gaps or filler words\n"
        "- Only create objects for meaningful, content-rich One Piece-specific narration\n"
        "- SKIP generic/filler segments entirely (e.g., 'get this', 'trick question', 'but here's the question', 'it's both', 'wait', 'and', 'so', 'then', 'but that's not all', or any line with less than 4 words)\n"
        "- Natural gaps between segments should be preserved as-is for pacing\n"
        "- Each object should represent ONE complete thought/concept with its original timing\n\n"
        "Use the exact schema below.\n\nSchema:\n[\n  {\n    \"start_time\": \"string\",\n    \"end_time\": \"string\",\n    \"summary\": \"string\",\n    \"image_search_query\": \"string\"\n  }\n]"
    )

    try:
        response = model.generate_content(prompt)
        if response.text:
            logger.info(f"Gemini raw response: {response.text}")
            import json as _json
            text = response.text.strip()
            if text.startswith("```"):
                text = text.split('\n', 1)[-1]
                if text.endswith("```"):
                    text = text.rsplit('```', 1)[0]
                text = text.strip()
            try:
                slides = _json.loads(text)
                slides = filter_irrelevant_summaries(slides)
            except Exception as parse_err:
                logger.error(
                    f"Failed to parse Gemini response as JSON: {response.text}")
                raise
        else:
            logger.error("No text generated by Gemini API")
            raise ValueError("No text generated by Gemini API")
    except Exception as e:
        logger.error(f"Error from Gemini API: {str(e)}")
        raise
    with open(out_path, 'w', encoding='utf-8') as f:
        json.dump(slides, f, indent=2)
    # Automatically download images if image_dir is provided
    if image_dir:
        download_images_for_slides(out_path, image_dir)
    return out_path


def generate_image_slides(ass_path: str, out_path: str, image_dir: str = None) -> str:
    """
    Backward-compatible alias for generate_gemini_image_slides.
    """
    return generate_gemini_image_slides(ass_path, out_path, image_dir)


def google_image_search(query, api_key=None, cse_id=None, num_results=10):
    api_key = api_key or os.getenv("GOOGLE_API_KEY")
    cse_id = cse_id or os.getenv("GOOGLE_SEARCH_ENGINE_ID")
    url = "https://www.googleapis.com/customsearch/v1"
    params = {
        "q": query,
        "cx": cse_id,
        "key": api_key,
        "searchType": "image",
        "num": num_results,
        "safe": "active"
    }
    resp = requests.get(url, params=params)
    resp.raise_for_status()
    results = resp.json()
    if "items" in results and results["items"]:
        return results["items"]
    return []


def download_image(url, save_path):
    """Downloads an image from a URL to a given path."""
    try:
        resp = requests.get(url, stream=True, timeout=10)
        resp.raise_for_status()
        with open(save_path, "wb") as f:
            for chunk in resp.iter_content(1024):
                f.write(chunk)
    except requests.exceptions.RequestException as e:
        logger.error(f"Failed to download image from {url}: {e}")
        raise

def download_images_for_slides(json_path, out_dir):
    # Clear the output directory before downloading new images
    if os.path.exists(out_dir):
        shutil.rmtree(out_dir)
    os.makedirs(out_dir, exist_ok=True)
    print(f"Downloading images to {out_dir} from {json_path}")
    logger.info(f"Downloading images to {out_dir} from {json_path}")
    with open(json_path, "r", encoding="utf-8") as f:
        slides = json.load(f)
    for idx, slide in enumerate(slides):
        search_query = slide.get("image_search_query")
        logger.info(f"[Slide {idx+1}] Image search query: {search_query}")
        if not search_query:
            logger.warning(f"No image_search_query for slide {idx+1}")
            continue

        try:
            if "one piece" not in search_query.lower():
                search_query = f"{search_query} One Piece Luffy"
            
            items = google_image_search(search_query, num_results=10)
            logger.info(f"[Slide {idx+1}] Google image search returned {len(items)} results.")

            # Filter for actual images with correct aspect ratio
            valid_images = []
            for item_idx, item in enumerate(items):
                item_link = item.get('link', 'N/A')
                item_mime = item.get('mime', 'N/A')
                item_width = item.get('image', {}).get('width', 0)
                item_height = item.get('image', {}).get('height', 0)

                if 'image' not in item_mime:
                    logger.debug(f"[Slide {idx+1} - Item {item_idx+1}] Skipping: Not an image MIME type ({item_mime}) - {item_link}")
                    continue
                
                if not item_width or not item_height:
                    logger.debug(f"[Slide {idx+1} - Item {item_idx+1}] Skipping: Missing dimensions ({item_width}x{item_height}) - {item_link}")
                    continue

                aspect_ratio = item_width / item_height
                if abs(aspect_ratio - 16/9) / (16/9) < 0.10: # 10% tolerance
                    valid_images.append(item)
                    logger.debug(f"[Slide {idx+1} - Item {item_idx+1}] Accepted: Aspect ratio {aspect_ratio:.2f} - {item_link}")
                else:
                    logger.debug(f"[Slide {idx+1} - Item {item_idx+1}] Skipping: Aspect ratio mismatch ({aspect_ratio:.2f} vs 16/9) - {item_link}")

            downloaded_successfully = False
            if valid_images:
                logger.debug(f"[Slide {idx+1}] Found {len(valid_images)} 16:9 images. Attempting download.")
                for item_idx, best_image in enumerate(valid_images):
                    img_url = best_image['link']
                    ext = os.path.splitext(img_url)[-1].split("?")[0] or ".jpg"
                    save_path = os.path.join(out_dir, f"slide_{idx+1}_16x9{ext}")
                    try:
                        logger.info(f"[Slide {idx+1}] Attempting to download 16:9 image {item_idx+1}: {img_url}")
                        download_image(img_url, save_path)
                        print(f"Downloaded: {save_path}")
                        logger.info(f"Downloaded: {save_path}")
                        downloaded_successfully = True
                        break # Exit loop if download is successful
                    except Exception as download_e:
                        logger.warning(f"[Slide {idx+1}] Failed to download 16:9 image {item_idx+1} from {img_url}: {download_e}")
            
            if not downloaded_successfully and items:
                logger.warning(f"[Slide {idx+1}] No suitable 16:9 image downloaded. Attempting fallback to any image.")
                for item_idx, fallback_item in enumerate(items):
                    img_url_fallback = fallback_item['link']
                    item_mime = fallback_item.get('mime', '')
                    if 'image' not in item_mime:
                        logger.debug(f"[Slide {idx+1} - Fallback Item {item_idx+1}] Skipping: Not an image MIME type ({item_mime}) - {img_url_fallback}")
                        continue
                    
                    ext = os.path.splitext(img_url_fallback)[-1].split("?")[0] or ".jpg"
                    save_path_fallback = os.path.join(out_dir, f"slide_{idx+1}_fallback{ext}") # Use a different name for fallback
                    try:
                        logger.info(f"[Slide {idx+1}] Attempting to download fallback image {item_idx+1}: {img_url_fallback}")
                        download_image(img_url_fallback, save_path_fallback)
                        print(f"Downloaded (fallback): {save_path_fallback}")
                        logger.info(f"Downloaded (fallback): {save_path_fallback}")
                        downloaded_successfully = True
                        break # Exit loop if download is successful
                    except Exception as fallback_e:
                        logger.warning(f"[Slide {idx+1}] Failed to download fallback image {item_idx+1} from {img_url_fallback}: {fallback_e}")

            if not downloaded_successfully:
                logger.error(f"[Slide {idx+1}] No image could be downloaded for query: {search_query}")
                print(f"No image could be downloaded for slide {idx+1}")

        except Exception as e:
            print(f"Failed to download for slide {idx+1}: {e}")
            logger.error(f"Failed to download for slide {idx+1}: {e}")
        finally:
            time.sleep(1) # Rate limit our requests
